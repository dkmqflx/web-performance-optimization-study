# 1장. **블로그 사이트 최적화**

## 1.1 실습 내용 소개

- 실적으로 배우는 웹 성능 최적화 for React

### 실습내용

- 이미지 사이즈 최적화

  - 이미지 사이즈가 너무 크면 서비스가 무거워지고

  - 이미지 사이즈가 너무 작으면 사용자가 저화질의 이미지를 보게 되서 불편함을 느낄 수 있다

  - 어떤 사이즈 이미지가 적절한지 배운다

- Code Split

  - 코드를 분할 하는 것

  - 어떻게 해야 코드를 분할할 수 있는지

  - 언제 코드를 분할해야 하는지

- 텍스트 압축

  - 어떤 페이지에 접속하게 되면 다양한 리소스를 다운 받게 된다

  - JS, CSS, HTML 등

  - 이런 리소스들을 다운 받기 전에, 미리 서버에서 압축해서 다운 받게 된다

  - 그러면 작은 사이즈로 서버에서 리소스를 다운 받을 수 있게 된다

  - 그렇게 웹 페이지는 빠르게 로드가 될 수 있다.

  - 이렇게 다운로드 받는 데이터 양을 줄여서 로딩 성능을 최적화 하는 것이다

- → 위의 세가지는 로딩 성능 최적화

- → 이렇게 세가지가 다운로드하는 데이터 양을 줄여서 로딩 성능을 최적화 하는 방법

- Bottlenet 코드 최적화

  - 내가 어떤 서비스를 개발했는데 특정 자바스크립트 코드 때문에 서비스가 너무 느리게 로드 되고 실행이 되는 경우,

  - 그럴 때 어떤 코드가 무엇 때문인지 몰라서 해메게 되는 경우가 있는데

  - 그런 병목 현상을 일으키는 코드를 어떻게 찾아내고 어떻게 최적화 하는지를 배운다

  - → 렌더링 성능 최적화

---

- Q. 코드 스플릿

  - 로딩 성능 최적화 중에 code split 과정이 있는데, 코드 분할이 왜 로딩 성능을 최적화 할 수 있는지 잘 모르겠습니다.

  - 서버에서 리소스를 다운받을 때 하나의 큰 파일을 받는것이 아니라 큰 파일을 여러가지 작은 파일들로 나누고 모듈화해서 다운받는 속도를 높일 수 있다는 뜻으로 이해하면 될까요??

- A. 코드 분할(Code Splitting)에 대해서 질문을 주셨는데요.

  - 코드 분할을 하지 않고 하나의 번들로 사용을 할 때는 처음 로딩부터, 당장은 필요하지 않은 모달 코드를 포함해서, 모든 코드를 불러오게 됩니다.

  - 하지만, 코드 분할을 하게 되면, 처음에는 당장 필요 없는 모달 코드를 제외한, 당장 필요한 코드 먼저 로드하게 되기 때문에 더 빠르게 첫 화면을 렌더링 할 수 있게 됩니다.

  - 예를 들어, 사이즈가 1000KB인 하나의 bundle.js로 된 서비스가 있다고 가정할 때, 이 중 200KB가 모달 관련된 코드라면,

  - 코드 분할은 이 번들을 bundle.js (800KB) / modal.js (200KB) 이렇게 만들어 줍니다.

  - 즉, 코드 분할 전에는 최초 화면을 렌더링하기 위해서 1000KB를 전부 로드해야 했지만,

  - 코드 분할을 통해 800KB의 코드만 로드하면 최초 화면을 렌더링 할 수 있게 되는겁니다.

  - (당연히 1000KB를 로드하는 것 보다, 800KB를 로드하는게 더욱 빠르게 로드할 수 있겠죠?)

  - 물론, 모달 기능을 사용할 때에는 결국 200KB를 로드할 필요가 있습니다.

  - 이 경우, 모달 코드를 뒤 늦게 로드하게 되는 것이므로 오히려 이 포인트(모달을 띄우는 순간)에서 약간의 딜레이(200KB의 모달 코드를 로드하는 시간)가 생길 수 있습니다.

  - preload는 이런 문제를 해결하기 위해서 적용을 할 수 있습니다.

  - 모달 기능을 사용하는 순간에 모달 코드를 로드하는 것이 아니라, 더 이전의 순간(모달 띄우는 버튼에 마우스를 올리는 순간,

  - 또는 첫 페이지가 모두 렌더링 된 후 메인 쓰레드에 여유가 있는 순간)에 모달 코드를 로드함으로써 최대한 분할된 모달 코드의 로드 시간을 단축시킬 수 있습니다.

---

## 1.2 분석 툴 소개 & 코드 다운로드

- 크롬 Network 탭

  - 네트워크 리소스들의 상세한 정보를 알려준다

- 크롬 Performance 탭

  - 웹 페이지가 동작할 때 실행되는 모든 작업들을 스크린샷, 그래프로 보여준다

- 크롬 Audit 탭 (Light house)

  - 우리 서비스가 성능적으로 어느정도 수준인지를 알 수 있다

  - 점수를 매겨주고 그에대한 가이드라인도 알려준다

- webpack-bundle-analyzer

  - 웹팩 라이브러리

  - 웹팩을 통해 번들링된 파일들이 어떤 코드를 갖고 있는지를 한눈에 보여주는 툴이다.

---

## 1.4 Audits 툴을 이용한 페이지 감사

- Audits(Light House)

  - 웹앱의 품질 개선을 위해 성능을 검사해주고 그 성능 향상을 위한 가이드를 제공해주는 도구

- Performance

  - 페이지의 성능을 나타내는 점수

- **OPPORTUNITIES, DIAGNOSTICS**

  - 웹 페이지의 문제점과 문제점을 해결하기 위한 가이드를 제시해준다.

- **OPPORTUNITIES**

  - 리소스의 관점에서 가이드를 제공해준다

  - 즉, 로딩 성능 최적화와 연관이 있다

- **DIAGNOSTICS**

  - 페이지의 실행 관점에서 가이드를 제공해준다

  - 즉, 렌더링 성능 최적화와 연관이 있다

  - 회색항목은 문제가 되는 것은 아니지만 한번 살펴볼 필요는 있다는 것

- **PASSED AUDITS**

  - 우리가 잘 적용한 항목을 의미한다.

- Runtime Setting

  - 우리가 검사할 때 사용한 환경을 요약해서 보여주는 표

---

- Q. 깃헙에서 클론한 같은 프로젝트인데도 결과가 다르게 나오는 이유가 무엇일까여

- A. 강의의 검사 결과와 직접 검사한 결과가 상이한 이유에 대해 질문을 주셨는데요, 말씀해주신 것 처럼 네트워크 및 CPU (또는 GPU) 등의 환경에 따라 검사 결과가 달라집니다.

  왜냐하면, 크롬에 달려있는 Lighthouse 는 결국 로컬 PC에서 검사를 진행하기 때문에 PC 환경에 영향을 받을 수 밖에 없기 때문입니다.

  추가적인 예를 들면, 아래와 같은 상황에서는 같은 페이지여도 다른 결과가 나올 수 있습니다.

  - CPU의 작업량에 여유가 있는 상태 vs CPU가 바쁘게 돌고 있는 상태(여러 프로그램을  띄워둔 상태)

  - 미국에 있는 서버 홈페이지를 한국의 사용자가 검사할 때 vs 미국의 사용자가 검사할 때

  - 웹서버에 트래픽이 많은 상황에서 검사를 했을 때 vs 트래픽이 없는 상황에서 검사를 했을 때

  - 그래서 검사 결과를 절대적인 지표로 삼기 보다는 하나의 가이드로 생각하시는게 좋습니다. 또한, 필요에 따라 한 번이 아닌 여러번 검사를 돌려보는 것도 좋은 방법입니다.

  - 참고하실 수 있는 링크를 하나 남겨드리겠습니다.

    - 구글에서 정리한 "[Lighthouse 결과의 변동성](https://developers.google.com/web/tools/lighthouse/variability)"에 대한 글입니다.

---

## 1.5 이미지 사이즈 최적화

### Properly size images

- Serve images that are appropriately-sized to save cellular data and improve load time.

  - 이미지를 적절한 사이즈로 압축해서 셀룰러 데이터와 로드 타임을 향상시켜라

- 이미지 사이즈와 최적화 했을 때 얼마만큼 줄일 수 있는지 확인할 수 있다.

- 이미지 목록에 있는 것을 클릭하면 Element 탭으로 이동해서 이미지를 확인할 수 있다

- 예를들어, Rendered size: 120 × 120 px, Intrinsic size: 1200 × 1200 px 인 상황을 가정해보자

- 넓이로 따지면 우리가 실제 필요한 사이즈보다 100배보다 큰 이미지를 사용하고 있는 상황이다.

- 이 때 이미지를 화면에 표시되는 이미지의 사이즈로 꼭 불러와야 하는 것은 아니다

- 요즘 많이 쓰는 레티나 디스플레이는 같은 공간에 더 많은 픽셀을 그릴 수 있기 때문에 너비 기준으로 2배 정도 큰 이미지를 사용하는 것이 적절다.

- 즉, 240 x 240 px이미지를 사용하는게 적절하다

- 이 이미지를 줄일 수 있는 방법 ?

  - 현재 이 이미지는 api를 통해서 받아오고 있다

  - 네트워크 탭의 articles의 response 에서 확인해보면, image라는 프로퍼티의 값으로 이미지를 받아오고 있다.

  - 이 이미지가 만약 자체 서버에 저장되어 있는 static 이미지라면 직접 이미지를 잘라서 올리면 되는데 api를 통해서 이미지를 받아오는 경우 어떻게 줄일 수 있을까 ?

  - 여기서 생각할 수 있는게 cloudinary나 imgix 같은 이미지 CDN을 사용하는 것이다

### CDN

- Contents Delivery Network

- 물리적 거리의 한계를 극복하기 위해 소비자(사용자)와 가까운 곳에 컨텐츠 서버를 두는 기술

- 한국에 있는 사용자가 미국에 있는 서버에 이미지를 다운 받을 때 아무리 인터넷이 빨라졌다고 해도, 사용자와 서버 사이에는 엄청난 물리적 거리가 있기 때문에 다운로드 받는 시간이 상당히 많이 걸린다

- 미국에 있는 서버를 미리 한국에 있는 서버에 복사해두고 사용자가 사진을 다운로드 할 때 한국에 있는 서버에서 바로 다운로드 해온다면 물리적 거리가 가까우니까 시간도 많이 줄어들 것이다

### Image CDN

- 하지만 이 CDN과 Image CDN은 조금 다르다

- 정확히는 Image Processing CDN이라고 하는데, Image CDN은 기본적인 CDN 의 개념과 이미지를 사용자에게 보내기 전에 특정형태로 가공해서

- 예를 들면 사이즈를 줄이거나 이미지 포맷을 마꾸거나 하는 처리 과정을 거쳐서 사용자에게 이미지를 전달하게 된다.

- 즉 서버에서 1200 px 사이즈의 이미지를 120px로 가공해서 사용자에게 전달해주는 것이다.

- Image CDN의 예시

  ```shell
  http://cdn.image.com?src=[img src]&width=200&height=100

  # img src : 내가 전달하고자 하는 이미지 소스
  # &width=200&height=100 : 내가 원하는 결과 이미지 정보를 parameter로 전달
  # 원본 이미지가 가공된다.
  ```

- [브런치](https://brunch.co.kr/)의 한 이미지를 예로 들어 보자

```shell
https://img1.daumcdn.net/thumb/C240x0/?fname=http://t1.daumcdn.net/brunch/service/user/9tkU/image/MqdiMMegItU002H1o44CNzdXXSg.png

# daumcdn 이라는 cdn 도메인이 있고
# fname= 필드로 어떤 이미지의 주소가 붙게 된다

```

```shell
http://t1.daumcdn.net/brunch/service/user/9tkU/image/MqdiMMegItU002H1o44CNzdXXSg.png

# 원본 이미지로 들어가 보면 이전과 달리 이미지의 크기가 더 큰 것을 확인할 수 있다.

```

- 즉, 이미지 CDN을 거쳐서 사용자에게 작은 사이즈로 제공되는 것을 확인할 수 있다.

- 여기서는 이미지 CDN을 직접 구축했지만, 직접 구축하지 않고 이미지 CDN 솔루션을 사용할 수 있다.

- 대표적으로 imgix라는 서비스가 있다

- 이 서비스를 사용하면 CDN을 구축하지 않아도 된다.

- 이 강의에서는 이미지 CDN을 사용하지 않는다. 이 강의에서는 최적화 포인트를 찾는데 중점을 두기 때문

- 여기서는 코드를 통해 문제를 해결한다.


- `components/Article/index.js`에 보면 img 태그가 있는 것을 확인할 수 있다.

```js
<img
  src={
    props.image +
    getParametersForUnsplash({
      width: 1200,
      height: 1200,
      quality: 80,
      format: 'jpg',
    })
  }
  alt='thumbnail'
/>
```

- 이미지를 넣는 것 뿐 아니라 width와 height를 정하고 있다.

- 이를 통해 앞서 Element 탭에서 확인한 이미지가 이 width와 height를 통해서 정해진 것을 알 수 있다

- 여기 있는 숫자 값을 변경하면 이미지의 사이즈도 줄어든 다는 것을 알 수 있다

```js
<img
  src={
    props.image +
    getParametersForUnsplash({
      width: 240,
      height: 240,
      quality: 80,
      format: 'jpg',
    })
  }
  alt='thumbnail'
/>

// unsplash에 데이터를 요청하는데, unsplash가 이미지 CDN 역할을 한다고 볼 수 있다.
```

- 새로고침 후 다시 Element 탭에서 확인해보면 240 x 240 px로 이미지 크기가 줄어든 것을 확인할 수 있다.

- 다시 lighthoutse 탭에서 Generate report 클릭 한 후 결과를 확인해본다

- OPPORTUNITIES 항목을 확인해보면 Performance 점수는 거의 변한게 없는데, Properly size images 항목이 없어진 것을 확인할 수 있다

- 즉 이미지에 대한 성능저하가 사라진 것을 확인할 수 있다

- 이렇게 성능을 한단계 더 향상시킬 수 있다

---

- Q.화면에 맞는 이미지 사이즈

  - 화면 사이즈에 비례해서 이미지 크기가 줄었다가 늘었다가 하는 경우에는 어떻게 사이즈 측정을 해서 받아오면 좋을까요?

  - 예를 들어 작은 사이즈를 받아왔다가 화면을 늘렸을 경우 사진이 깨질것 같은데 이런 경우에는 srcset과 같은 속성을 사용하면 괜찮을까요?

- A. 반응형 이미지와 관련하여 질문을 주셨는데요,

  - 이번 강의에서 반응형에 대한 이미지 사이즈 조절 내용은 다루지 않았지만,

  - 만약 이미지를 조금 더 효율적으로 불러오기 위해 작은 화면에서는 작은 이미지를 로드하고,

  - 큰 화면에서는 큰 이미지를 로드하는식으로 반응형을 적용하고 싶다면,

  - 언급하신 img 태그의 srcset과 sizes 속성을 사용하여 반응형을 구현할 수 있습니다.

  - 다른 방법으로는 picture 태그(+source 태그)에서 srcset과 media 속성을 사용하여 구현할 수 있습니다.

  - 자세한 내용은 아래 문서를 참고하시면 도움이 되실 것 같습니다.
    (https://developer.mozilla.org/ko/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images#%EC%95%84%ED%8A%B8_%EB%94%94%EB%A0%89%EC%85%98)

  - 반응형 이미지에 대한 최적화 내용도 실습 코드와 함께 자세한 설명으로 다음 강의에 포함할 수 있도록 해보겠습니다.

  - 답변이 도움되었기를 바라며, 강의에 관심을 가져주셔서 감사합니다. :)

<br/>

- Q. 이미지 사이즈 최적화 질문 있습니다.

  - 안녕하세요. 이번 이미지 사이즈 최적화 강의를 보고 질문드립니다.

  - API 서버에서 원본 이미지를 원하는 사이즈로 최적화를 하는데 그만큼 시간적인 비용이 들어가잖아요.

  - 그러면 클라이언트 관점에서 소모되는 용량은 작아도 백엔드에서 이미지 사이즈 최적화하는 작업 시간 때문에 페이지가 로드되는 시간은 큰 차이가 없을 수 있지 않을까 생각하는데 백엔드에서 사이즈 최적화를 거친 이미지를 로드하는 것이 원본 이미지를 바로 로드하는 것보다 시간적인 비용이 더 저렴하다고 보면 되는 걸까요?

  - 사이즈 최적화 작업 + 최적화 된 이미지 응답 속도가 원본 이미지 응답 속도보다 빠르다고 보면 될지..

- A. 이미지 처리 서버의 효율성에 대해서 질문 주셨는데요,

  - 클라이언트에서 이미지를 요청할 때 마다 서버에서 이미지를 가공한다면 말씀하신 것처럼 시간도 오래걸리고 리소스 또한 많이 잡아먹습니다.

  - 그래서 이미지를 업로드할 때 필요한 이미지 형태에 대해서 미리 가공해두거나 아니면 한 번 가공된 이미지는 캐시를 해뒀다 다음 요청 때는 미리 가공해 뒀던 이미지를 제공해 줍니다.

  - 답변이 도움되었기를 바라며, 강의에 관심을 가져주셔서 감사합니다.

<br/>

- Q. 이미지 화질에 대해 질문있습니다

  - 10배 크기의 이미지를 2배로 바꾼후 렌더링하니 화질이 많이 떨어져 보이게 됩니다.

  - 만약 화질이 중요시 되는 상황이라면 최적화를 맞추지 못하더라도 3배 4배의 크기로 이미지를 불러와 우선순위를 화질로 하는것이 옳은 방법인가요?

  - 아무래도 성능과 품질에 대한 타협점은 상황에 따라 맞춰가는게 맞겠죠??

- A. 이미지 사이즈에 대해서 질문을 주셨는데요,

  - 10배 크기의 이미지를 2배로 바꿨다는게 어떤 의미일까요?

  - 10 만큼의 크기인 이미지를 2로 이미지 자체를 압축해서 렌더링 했을 때를 말씀하시는 걸까요?

  - 만약 그렇다면, 말씀하신대로 성능과 품질에 대한 조율을 하시면 될 것 같습니다.

  - 예를 들어, 이미지 갤러리 서비스와 같이 이미지가 메인인, 혹은 이미지의 선명함이 중요한 서비스를 개발한다고 하면, 이미지 로딩이 조금 오래 걸리더라도 고화질의 이미지를 넣는 것이 좋고,

  - 그런게 아니라 사소한 이미지일 경우, 굳이 고화질의 이미지로 넣어주실 필요가 없습니다.

  - 고화질의 이미지를 넣을 때에도 몇 가지 팁이 있는데,

    - 가능하면 PNG 대신 JPG 또는 WEBP 로 압축

    - 이미지 로드 전후에 Layout Shift가 발생하지 안도록 영역 잡아두기

    - 이미지 로딩에 대한 UX 적용 (스켈레톤, 로더 등)

  - 등등 이렇게 적용을 하시면 이미지 로드가 오래걸려도 사용성을 크게 해치지 않을 것이라고 생각됩니다.

  - 추가로, 일반적으로 이미지는 실제 화면에 렌더링 되는 사이즈의 2배 이미지로 넣어주시는게 최적입니다. (ex - 100x100 으로 화면에 넣어줘야 한다면, 200x200 이미지 사용)

  - 자세한 내용은 아래 링크를 참고하시면 좋습니다.

    - https://www.danrodney.com/blog/retina-web-graphics-explained-1x-versus-2x-low-res-versus-hi-res/

    - https://www.sleeplessmedia.com/2018/12/14/optimizing-website-images-and-graphics-for-retina-displays/

  - 추가 질문

    - Q. 강의에서 120px로 이미지를 렌더링하기위해 unsplash API로 1200px이미지를 불러왔던것을 최적화를 위해 240px로 줄이게 돼서 10배에서 2배로 바꿨다고 표현했는데 설명이 부족했던것 같습니다

    - A. 그런데 4배의 이미지는 의미가 없는 걸로 알고 있습니다.

      - 결국 렌더링 하는 영역은 정해저 있으니까요.

      - 만약 3x이나 4x인 레티나 디스플레이에 대해서 대응을 한다면 의미가 있겠습니다만,,,
        (좀 찾아보니 모바일은 3x 까지도 사용되는 것 같네요, 3배 이미지까지는 의미가 있을 것 같습니다.)

---

## 1.6 bottleneck 코드 탐색

- OPPORTUNITIES와 DIAGNOSTIC 항목을 살펴보고 느린 자바스크립트를 어떻게 하면 최적화 할 수 있는지 알아본다

### 1. OPPORTUNITIES 항목

- Minify JavaScript

  - 자바스크립트 코드 중에 공백, 주석등을 제거함으로써 자바스크립트 파일의 크기를 줄이는 것을 의미

  - CRA의 경우 production build를 하면 알아서 Minify를 해주기 때문에 여기서 우리가 직접 신경쓰지 않아도 된다

- Preconnect to required origins

  - 여기서 의미하는 것은 미리 우리가 사용할 리소스를 pre connect 하고 DNS를 pre fetch 하라는 것 (다음 강의에서 언급할 것이므로 여기서는 건너 뛴다)

### 2. DIAGNOSTICS 항목

- Minimized main-thread work

  - 메인 스레드의 작업을 줄이라는 의미

  - 항목을 클릭하면 어떤 작업에서 시간이 많이 걸렸는지를 보여준다

  - 하지만 무엇 때문에 그렇게 오랜 시간이 걸렸는지는 알 수 없다.

- Serve static assets with an efficient cache policy

  - 캐시를 하라는 의미의 항목

  - 클릭하면 캐시가 걸려 있지 않는 항목을 볼 수 있다.

  - 여기서는 자바스크립트를 최적화 하니까 건너 뛴다

- Reduce JavaScript execution time

  - 자바스크립트의 실행시간을 줄여라

  - 각 자바스크립트 파일이 실행되면서 얼마나 소요되는지 확인할 수 있다

  - `chrome-`으로 시작하는 부분은 이 프로그램과 상관없는 크롬의 확장 프로그램이므로 신경쓰지 않아도 된다.

- 여기서 `Minimized main-thread work` 항목과 `Reduce JavaScript execution time` 항목을 통해 알 수 있는 것은 자바스크립트 코드가 굉장히 오래 걸린다는 것이다.

- 하지만 어떤 코드 때문에 오래 걸렸는지는 정확히 알 수 없다

- 이 때 필요한 것이 `Performance` 탭

- Performance

  - 페이지가 로드 되면서 실행되는 작업들을 타임라인과 차트 형태로 보여준다

  - 사용법은 `Start profiling and reload page` 버튼을 클릭해주면 된다.

- 맨 위에 있는 줄은 어떤 타입의 작업들이 어느 정도의 비율로 실행되었는지를 보여준다.

- 그리고 페이지 전체의 타임라인을 의미한다. (ex. 499ms ~ 6999ms)

- 그래서 여기서 우리가 원하는 스크립트만을 선택적으로 볼 수 있다. (click or drag)

- 더블클릭하면 전체 영역을 한번에 선택할 수 있게 된다.

- 아래에서는 위에서 선택한 영역에서 벌어지는 Frame 형태로 상세하게 보여주게 된다

- Network, Frames, Timings, Main 등 이러한 것을 Framce 차트라고 한다.

  - Network : 네트워크 타임라인

  - Frames, Timings, Main : 자바스크립트의 실행과 관련된 작업이 나열 된다

- 가장아래, Summary 탭이 있는 부분은 상세 내용이 있는 곳

- 선택한 영역 또는 작업에 대한 상세한 내용을 볼 수 있는 화면

- network 가장 처음 부분 클릭해서 summary 부분 보면 text/html,

- 즉 html 파일을 받아오는 것을 확인할 수 있다

- 그리고 html 파일이 로드 되니까 뒤에 .js로 끝나는 총 3개의 자바스크립트 파일도 로드되는 것을 확인할 수 있다

- 0.chunk.js 파일을 받는데 시간이 굉장히 오래 걸렸다

- 대략 0.4초 정도 걸렸다

- 상세 내용 보면 사이즈가 995kb로 상당히 크다

- Network 탭의 html 로드가 끝나는 시점에 Main 탭의 Parsing html이 시작되는 것을 확인할 수 있는데, html을 받은 다음에 html 파일을 해석하는 것을 확인할 수 있다

- 이와같이 Network 탭의 모든 자바스크립트 로드가 끝난 다음에 Main 탭을 보면 굉장히 많은 자바스크립트 코드가 실행되는 것을 확인할수 있다

- 여기서 실행되는 코드는 우리가 만든 컴포넌트들이다.

- 자바스크립트 코드의 실행이 끝난 시점에 파란색, 초록색, 빨간색 (DCL, FP, TCP, L ) 선이 그어져 있는 것을 확인할 수 있다

- 이것은 Performance 탭에서 분석하기 편하라고 일종의 가이드를 주는 것으로 마우스를 오리면 어떤 항목을 의미하는지 알 수 있다

- 즉, 실제로 화면에 그려지는 시간의 구간을 의미한다

- 그리고 그 구간 위에 네트워크 탭을 보면 어떤 워크 통신이 일어나고 있는 것을 확인할 수 있는데 articles api 주소를 확인할 수 있다

- 페이지 로드가 끝나고 api 요청하는 시점이 그 시점인 것을 알 수 있다

- api 요청이 끝났는데 뒤에 선이 계속해서 이어지는 것이 의미하는 것은 api의 콜백을 의미한다.

- 콜백이 실행되는 시간이 그렇게 오래걸린다는 것이다

- 그리고 아래 자바스크립트 코드를 보면 (Timings, Main) 병목 현상을 이르키는 자바스크립트 코드에 가까워지고 있다

- Main 보면 article 컴포넌트가 굉장히 오랫동안 이어지고 있는데 이를 통해서 어떤 문제가 있구나 라는 것을 짐작할 수 있다

- 또한 리액트는 Timings에서 컴포넌트를 확인할 수 있는데, 보면 Article 컴포넌트가 굉장히 오랫동안 이어지는 것을 확인할 수 있다

- Article 컴포넌트에서 무슨일이 일어나기에 이렇게 오랜 시간이 걸리는 걸까

- 그 답은 Main 탭을 보면 Articles 컴포넌트에 `removeSpecialCharacter` 라는 작업을 실행하는 것을 확인할 수 있느데 이 작업이 컴포넌트의 모든 시간을 잡아 먹고 있었다

- `removeSpecialCharacter` 여러개로 쪼개져 있는데 실제로 `removeSpecialCharacter` 작업이 여러번 실행되는 것이 아니라 하나의 연결된 작업 이었는데 너무 많은 작업 리소스를 사용하다 보니 중간 중간에 가비지 컬렉터에 의해서 중간중간 끊긴 것이다

- 아래에 중간 중간 있는 Minor GC를 보고 알 수 있는데 Minor GC는 가바지 컬렉터의 작업인데 메모리에 여유가 없을 때 메모리 정리를 해주는 작업

---

- Q. 퍼포먼스탭에서 api 요청~ 콜백 까지의 시간 병목 판단 기준 질문드립니다

  - 해당 편에서 1300ms~2400ms 까지 콜백 시간이 길게 늘어져서 병목이 일어난다고 판단된다고(비슷하게) 말씀해주셨는데요.

  - 혹시 실무 라이브 서비스를 기준으로 봤을때 몇ms 정도를 기준으로 병목인지 아닌지를 판단할수 있을까요?

- A. 실무에서 어느 정도의 delay를 병목으로 판단하는지 질문해주셨는데요,

  - 결론적으로는, 너무나 당연하게, 회사에 따라 또는 팀에 따라 그 기준이 다르며, 대부분은 그 기준이 명확하지 않습니다.

  - 먼저, delay 라고 하면, 화면에 노출되는 시간을 의미하는데요,

  - 이 시간은 절대적일 수 없고 페이지를 로드하는 환경에 따라 달라지기 때문에 명확하게 정할 수가 없습니다.

  - Lighthouse의 기준을 보면, FCP(First Contentful Paint) 의 경우, 다음과 같은 기준으로 점수를 측정합니다.

    - FCP time(in seconds) / Color-coding / FCP score (HTTP Archive percentile)

    - 0–2 Green / (fast) / 75–100

    - 2–4 Orange / (moderate) / 50–74

    - Over 4 Red / (slow) / 0–49

    - [출처](https://web.dev/first-contentful-paint/)

  - 그 외의 항목들에 대한 성능 기준은 [아래 링크](https://web.dev/lighthouse-performance/#metrics)에서 확인하실 수 있습니다.

  - 위와 같은 기준으로 봤을 때, 구글에서는

  1. 3G 환경에서,

  2. 화면이 뜨는데 2초,

  3. 사용자가 상호작용 하기까지 5초,
     를 권장하고 있습니다.

  - 추가로 강의의 콜백시간은 순전히 자바스크립트의 실행시간이지만, 위에서 권장하는 시간은 자바스크립트 실행시간 뿐만 아니라 네트워크 및 화면 렌더링 시간도 포함한 시간입니다.

<br/>

- Q. performance 중 회색바..!

  - 뒤로 이어지는 회색바는 articles의 콜백을 의미한다고 얘기해주셨는데 앞에있는 회색바는 어떤의미로 보면 될까요?

  - articles를 다운받기 시작하고 종료하는 시점은 네모박스부분인것 같은데 앞에 있는데 회색바는 굳이 표시해야하는 이유가 있을까요

- A.네트워크 타이밍에 대해서 질문을 주셨는데요,

  - 퍼포먼스 패널에서 나타나는 왼쪽 회색 선은 요청을 보내기 직전까지의 준비 시간을 의미합니다.

  - 네트워크 패널에서 조금 더 자세히 볼 수 있는데요, (Network - Timing)

  - 아래 이미지는 특정 리소스에 대한 네트워크 타이밍 내용입니다.

  - 보시면, Request sent를 하기 전에 Queueing과 Stalled 라는 타이밍이 있는 것을 알 수 있는데, 해당 시간이 회색 선으로 나타납니다.

  - 쉽게 얘기해서, 네트워크 요청을 보내기 위한 준비 시간이라고 볼 수 있습니다.

<br/>

- Q. removeSpeical 함수를 줄여야겠다고 하는 이유!

  - 제가 이번 강의에서 궁금한게 10:24 쯔음, removeSpeicalCharacter 함수가 Article 컴포넌트 렌딩 시간의 대부분을 차지한다고 하셨고,

  - GC가 많이 작동되어 함수 실행이 끊긴거라고 말씀해주신걸로 기억합니다.

  - 혹시 이 중에서, GC가 많이 작동되어 함수 실행이 끊겼기 떄문에 아 removeSpeicalCharacter 함수를 리펙토링을 진행해야겠다고 판단할 수 있는 건가요?? 그 기준이 궁금합니다!

- A. 병목 함수에 관해서 문의를 주셨는데요,

  - 어떤 함수 문제를 일으키는지를 판단하는 방법은 여러가지가 있을 겁니다.

  - 몇가지 나열해보면 로직 자체가 비효율적이여서 시간을 많이 잡아먹는 경우도 있을 것이고, 메모리를 심하게 낭비하는 경우도 있을 겁니다.

  - 해당 예제의 경우는 비효율적인 로직으로 인해 시간과 메모리를 모두 낭비한 경우인데요, 메모리가 낭비되면서 가비지콜렉터도 많이 동작했고 그걸로 많은 병목이 생긴걸 알 수 있었습니다.

  - 물론 그 뿐만 아니라 애초에 함수가 실행되는 시간이 굉장히 길어서 가비지콜렉터가 아니더라도 알 수 있었겠죠.

  - 판단은 상황에 따라 다르겠지만 함수의 실행 시간이 비정상적으로 길다고 느껴지면 체크해볼 필요가 있습니다.

  - 그럼 답변이 도움되셨길 바라며, 강의에 관심을 가져주셔서 감사합니다. :)

---

## 1-7 bottleneck 코드 최적화

- Article 컴포넌트 가보면 `removeSpecialCharacter` 함수가 호출되는 것을 확인할 수 있다

- 이 함수의 코드를 보면 특수 문자의 리스트를 반복을 돌리면서 넘겨 받은 문자(str)에서 특수 문자가 있는지를 검사하고 만약 그렇다면 substr, concat 함수를 사용해서 특수 문자를 사용한다

- 특수 문자를 제거하는데 for 문 두번 사용하고 있다

- 자바스크립트에는 문자열을 제거해주는 replace 함수가 있다

- 여러번 반복문 사용할 필요없이 replace 함수 사용할 수 있다

- 그리고 인자로 전달되는 str도 api에서 주는 굉장히 긴 마크다운 문자열일 텐데 이를 많은 반복과 비효율적인 함수를 통해서 처리하니 느릴 수 밖에 없다

### Bottleneck 해결방안

1. 특수 문자를 효율적으로 제거하기

- replace 함수와 정규식 사용 또는 

- 마크다운의 특수문자를 지워주는 라이브러리 사용(remove-markdown)
  
  - 이런 라이브러리 사용하면 효과적이고 깔끔한 결과 얻을 수 있지만, 이 강의에서는 직접 구현하는 첫번째 방법으로 최적화 해준다


2. 작업하는 양 줄이기

- 기본적으로 api를 통해 제공되는 마크다운 문자열의 길이는 굉장히 길다.

- 제일 긴것은 거의 9만자

- 이걸 전부 최적화 하기 위해 반복문을 돌리니 느릴 수 밖에 없다

- 하지만 우리가 필요한 것은 많은 양의 문자열이 아니라

- 목록에서 사용자에게 보여지는, 약 200자 정도의 문자열이다

- 따라서 200자, 많아봐야 300자만 검사하는 것만으로도 충분하다

- 코드를 수정하고 다시 Performance 탭에서 검사해보면 Article 컴포넌트와 removeSpecialCharacter 함수가 차지하는 크기가 이전에 비해 너무 작아져서 확인하기 힘들어진 것을 확인할 수 있다

- 시간으로 보면 함수를 한번 호출하는데 0.14ms가 걸리는 것을 확인할 수 있는데 이를 통해 굉장히 짧아진 것을 확인할 수 있다

- 또한 lighthouste에서 다시 검사해보면 Performance 점수가 굉장히 높아진 것을 확인할 수 있다

- 그리고 Diagnostics의 Minimize main-thread work, Reduce JavaScript execution time 항목도 줄어든 것을 확인할 수 있다



